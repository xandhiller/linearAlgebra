\documentclass{article}
\author{Alex Hiller}
\title{Assignment 3 - Linear Algebra, 2019}
\setlength{\parindent}{0cm}
\setlength{\parskip}{0.125cm}
\pagenumbering{gobble}
\usepackage[margin=2.25cm]{geometry} % Formatting
\usepackage{amsmath}                % Mathematics
\usepackage{amssymb}                % Mathematics
\usepackage{esint}                  % Mathematics
\usepackage{listings}               % Listings
\usepackage{color}                  % Listings
\usepackage{courier}                % Listings
\usepackage{circuitikz}             % Circuits
\usepackage{titlesec}               % Section Formatting
\usepackage{graphicx}               % Graphics
\usepackage{stmaryrd}               % \mapsfrom arrow. â†¤
\input{/home/polluticorn/GitHub/texTemplates/texMacros}
% Section formatting
\titleformat{\section}{\huge \bfseries}{}{0em}{}[]
\titleformat{\subsection}{\Large \bfseries}{}{0em}{}
\titleformat{\subsubsection}{\bfseries}{}{0em}{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Question 1}

Jacobi's method for iteration is given by:

\begin{equation}
  x^{k+1}_{i} = 
  \frac{1}{a_{ii}}
  \bigg( b_{i} - \sum^{n}_{j=1,j\neq i} a_{ij} x_{j} \bigg)
\end{equation}

Using an initial guess of $\mathbf{x}^{(0)} = \mathbf{0}$, the code in the 
appendix produces the following output for four iterations:

\begin{verbatim}
--------------------------------------------------------------------------------
x_(1):
-4.000000                       2.000000                        6.000000

relative precision w/ supremum norm is: 1.000000


x_(2):
2.000000                        7.000000                        9.333334

relative precision w/ supremum norm is: 0.642857


x_(3):
5.333334                        5.666667                        7.000000

relative precision w/ supremum norm is: 0.476191


x_(4):
3.000000                        2.833333                        4.333333

relative precision w/ supremum norm is: 0.538462
--------------------------------------------------------------------------------
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Question 2}

The Gauss-Siedel method for iteration is given by:

\begin{equation}
  x^{(k+1)}_{i} = 
  \frac{1}{a_{ii}}
  \bigg( b_{i} - \sum^{i-1}_{j=1} a_{ij} x^{(k+1)}_{j} - 
  \sum^{n}_{j=i+1} a_{ij} x^{(k)}_{j} \bigg)
\end{equation}

Using an initial guess of $\mathbf{x}^{(0)} = \mathbf{0}$, the code in the 
appendix produces the following output for three iterations:

\begin{verbatim}
--------------------------------------------------------------------------------
x_(1):
-4.000000                       4.000000                        10.000000

relative precision w/ supremum norm is: 1.000000


x_(2):
6.000000                        4.000000                        3.333333

relative precision w/ supremum norm is: 1.666667


x_(3):
-0.666667                       4.000000                        7.777778

relative precision w/ supremum norm is: 0.571429
--------------------------------------------------------------------------------
\end{verbatim}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Question 3}

The relaxation parameter method uses the value of $\omega$ to determine how much
to weight the previous value of $x^{(k)}$ and is given by the formula:


\begin{equation}
  x^{(k+1)}_{i} = 
  x^{(k)}_i +
  \frac{\omega}{a_{ii}}
  \bigg( b_{i} - \sum^{i-1}_{j=1} a_{ij} x^{(k+1)}_{j} - 
  \sum^{n}_{j=i} a_{ij} x^{(k)}_{j} \bigg)
\end{equation}


Setting $\omega = 0.5$ and using an initial guess of 
$\mathbf{x}^{(0)} = \mathbf{0}$, the code in the appendix produces the following 
output for one iteration:

\begin{verbatim}
--------------------------------------------------------------------------------  
x_(1):
-3.600000                       3.420000                        8.585999

relative precision w/ supremum norm is: 1.000000
--------------------------------------------------------------------------------  
\end{verbatim}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Question 4}

If the matrix is diagonally dominant, then it is known that Gauss-Siedel
and Jacobi methods will converge.

For our matrix, $\mathbf{A}$:

$$
\mathbf{A} = 
\begin{bmatrix}
  1 & 0 & -1 \\
  1 & 2 & -1 \\
  2 & -1 & 3 \\
\end{bmatrix} 
$$

We can see that it is diagonally dominant (but not strictly diagonally dominant).

And so it should converge for the Jacobi and Gauss-Siedel methods, which it does,
as we've seen in previous questions.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Question 5}
\subsection{Part (a)} 
For the system $\mathbf{Ax}=\mathbf{b}$, the relative number of iterations 
amongst the three methods are summarised below.
\begin{table}[!htbp]
  \centering
  \begin{tabular}{|c|c|} \hline
    \textbf{Method}                    &  \textbf{Number of Iterations}     \\ \hline
    Jacobi                             & 23 \\ \hline
    Gauss-Siedel                       & 25 \\ \hline
    Under Relaxation ($\omega = 0.5$)  & 17 \\ \hline
  \end{tabular}
\end{table}
\end{center}


\subsection{Part (b)} 
Here with $\mathbf{x}^{(0)} = \mathbf{0}$, the relative precision being 
$10^{-4}$ with the supremum norm and varying 
$w \in \{0.1, \ 0.2, \ ... \ ,\ 0.9\}$ we can see the optimal value for under-
relaxation is $\omega=0.8$.

\begin{table}[!htbp]
  \centering
  \begin{tabular}{|c|c|} \hline
    $\mathbf{\omega}$                  &  \textbf{Number of Iterations}     \\ \hline
    0.1  & 45 \\ \hline
    0.2  & 31 \\ \hline
    0.3  & 27 \\ \hline
    0.4  & 21 \\ \hline
    0.5  & 17 \\ \hline
    0.6  & 13 \\ \hline
    0.7  & 10 \\ \hline
    0.8  & 8  \\ \hline
    0.9  & 9  \\ \hline
  \end{tabular}
\end{table}
\end{center}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\clearpage
\section{Appendix}
\lstinputlisting[language=C]{threeMethods.c}

\end{document}
